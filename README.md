# finetune_google-gemma-2B_with_LORA
Fine-tuned Gemma-2B using LoRA with 4-bit quantization on the Abirate/english_quotes dataset. This project demonstrates parameter-efficient supervised fine-tuning with TRL’s SFTTrainer, enabling structured quote–author text generation while keeping GPU memory usage low and training efficient on a single Colab GPU environment.
